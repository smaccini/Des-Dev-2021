{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "print(\"Lets's start coding...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lets's start coding...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(\"Time for dictionaries!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time for dictionaries!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(\"And sorting!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "And sorting!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# For this exercise, you should start by reading in the text file produced from Exercise Three. \r\n",
    "# This text file should already have the HTML code elements removed, and primarily consist of text and other characters that we will remove through our processing. \r\n",
    "# Store the input of your file in a string, and convert the contents to lower case for consistency.\r\n",
    "\r\n",
    "# open the txt file to read it\r\n",
    "f = open(\"NPRarticlecleaned.txt\",\"r\")\r\n",
    "\r\n",
    "# create new variable with contents\r\n",
    "NPRtextWeek4 = f.read()\r\n",
    "print(NPRtextWeek4[0:200])\r\n",
    "f.close()\r\n",
    "\r\n",
    "# covert to lower\r\n",
    "\r\n",
    "NPRcleanedlower = NPRtextWeek4.lower()\r\n",
    "print(NPRcleanedlower[0:200])\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Torey Edmonds has lived in the same house in an African-American neighborhood of the East End of Richmond, Va., for all of her 61 years. When she was a little girl, she says her neighborhood was a pla\n",
      "torey edmonds has lived in the same house in an african-american neighborhood of the east end of richmond, va., for all of her 61 years. when she was a little girl, she says her neighborhood was a pla\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# This week, we'll be using functions that we've already defined in our exercises. \r\n",
    "# Try looking back through your code and notes from class. Our first step is to import the regular expressions module and remove all non-alpha-numeric characters from the string, \r\n",
    "# then save the string as an array of words ready for processing. \r\n",
    "# (Use the code in NormalizingText for reference)\r\n",
    "\r\n",
    "# In class, we did these in reverse order, and I am going to stick with the order used in class.\r\n",
    "\r\n",
    "# Creating the bag of words array\r\n",
    "NPR_word_bag = NPRcleanedlower.split()\r\n",
    "print(NPR_word_bag[0:200])\r\n",
    "\r\n",
    "\r\n",
    "# Remove non-alpha-numeric characters\r\n",
    "\r\n",
    "import re\r\n",
    "NPR_word_bag = re.compile(r'\\W+', re.UNICODE).split(NPRcleanedlower)\r\n",
    "print(NPR_word_bag[0:200])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['torey', 'edmonds', 'has', 'lived', 'in', 'the', 'same', 'house', 'in', 'an', 'african-american', 'neighborhood', 'of', 'the', 'east', 'end', 'of', 'richmond,', 'va.,', 'for', 'all', 'of', 'her', '61', 'years.', 'when', 'she', 'was', 'a', 'little', 'girl,', 'she', 'says', 'her', 'neighborhood', 'was', 'a', 'place', 'of', 'tidy', 'homes', 'with', 'rose', 'bushes', 'and', 'fruit', 'trees,', 'and', 'residents', 'had', 'ready', 'access', 'to', 'shops', 'like', 'beauty', 'salons,', 'movie', 'theaters', 'and', 'several', 'grocery', 'stores.', 'but', 'as', 'she', 'grew', 'up,', 'she', 'says,', 'the', 'neighborhood', 'went', 'downhill.', 'by', 'the', '1970s,', 'stores', 'had', 'disappeared;', 'those', 'that', 'did', 'return', 'were', 'corner', 'shops', 'selling', 'cheap', 'alcohol', 'but', '\"no', 'real', 'food,\"', 'edmonds', 'says.', 'houses', 'declined', 'too,', 'as', 'homeowners', '\\\\xe2\\\\x80\\\\x93', 'including', 'her', 'parents', '\\\\xe2\\\\x80\\\\x93', 'were', 'rejected', 'for', 'loans.', '\"if', 'the', \"bank\\\\'s\", 'not', 'loaning,', 'she', 'says,', '\"then', 'things', 'deteriorate.\"', 'today,', \"edmonds\\\\'\", 'neighborhood', 'remains', 'overwhelmingly', 'african-american,', 'with', 'a', 'poverty', 'rate', 'of', 'nearly', '60%.', 'many', 'of', 'her', 'neighbors', 'suffer', 'chronic', 'medical', 'conditions', 'like', 'kidney', 'disease', 'and', 'diabetes.', '\"they', 'age', 'differently,\"', 'says', 'edmonds,', 'who', 'works', 'for', 'virginia', 'commonwealth', 'university', 'promoting', 'community', 'health.', '\"we', 'have', 'a', 'lot', 'of', 'our', 'neighbors', 'that', 'have', 'health', 'challenges.\"', '\\\\n', '\\\\n\\\\n', '\\\\n\\\\n', '\\\\n', 'loading...', '\\\\n', '\\\\n', '\\\\n\\\\n', 'but', \"it\\\\'s\", 'not', 'just', \"edmonds\\\\'\", 'neighborhood.', 'in', 'city', 'after', 'city', 'across', 'the', 'u.s.,', 'from', 'milwaukee', 'to', 'miami,', 'researchers', 'have', 'found', 'a']\n",
      "['torey', 'edmonds', 'has', 'lived', 'in', 'the', 'same', 'house', 'in', 'an', 'african', 'american', 'neighborhood', 'of', 'the', 'east', 'end', 'of', 'richmond', 'va', 'for', 'all', 'of', 'her', '61', 'years', 'when', 'she', 'was', 'a', 'little', 'girl', 'she', 'says', 'her', 'neighborhood', 'was', 'a', 'place', 'of', 'tidy', 'homes', 'with', 'rose', 'bushes', 'and', 'fruit', 'trees', 'and', 'residents', 'had', 'ready', 'access', 'to', 'shops', 'like', 'beauty', 'salons', 'movie', 'theaters', 'and', 'several', 'grocery', 'stores', 'but', 'as', 'she', 'grew', 'up', 'she', 'says', 'the', 'neighborhood', 'went', 'downhill', 'by', 'the', '1970s', 'stores', 'had', 'disappeared', 'those', 'that', 'did', 'return', 'were', 'corner', 'shops', 'selling', 'cheap', 'alcohol', 'but', 'no', 'real', 'food', 'edmonds', 'says', 'houses', 'declined', 'too', 'as', 'homeowners', 'xe2', 'x80', 'x93', 'including', 'her', 'parents', 'xe2', 'x80', 'x93', 'were', 'rejected', 'for', 'loans', 'if', 'the', 'bank', 's', 'not', 'loaning', 'she', 'says', 'then', 'things', 'deteriorate', 'today', 'edmonds', 'neighborhood', 'remains', 'overwhelmingly', 'african', 'american', 'with', 'a', 'poverty', 'rate', 'of', 'nearly', '60', 'many', 'of', 'her', 'neighbors', 'suffer', 'chronic', 'medical', 'conditions', 'like', 'kidney', 'disease', 'and', 'diabetes', 'they', 'age', 'differently', 'says', 'edmonds', 'who', 'works', 'for', 'virginia', 'commonwealth', 'university', 'promoting', 'community', 'health', 'we', 'have', 'a', 'lot', 'of', 'our', 'neighbors', 'that', 'have', 'health', 'challenges', 'n', 'n', 'n', 'n', 'n', 'n', 'loading', 'n', 'n', 'n', 'n', 'but', 'it', 's', 'not', 'just', 'edmonds', 'neighborhood', 'in', 'city', 'after', 'city']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Next, we'll use stop words to remove all the words that we don't want to include in our count. \r\n",
    "# There are suggested words in this week's readings, but you can also generate a custom list based on your topic. Define the stop words in an array, \r\n",
    "# and use a loop to remove any word in your bag of words that also appears on the stop words list (use the examples in Dictionaries for guidance.)\r\n",
    "\r\n",
    "# Using stopwords from Turkel list plus adding a few more that seem to be clutter to my list\r\n",
    "\r\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\r\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\r\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\r\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\r\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\r\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\r\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\r\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\r\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\r\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\r\n",
    "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\r\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\r\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\r\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\r\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\r\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\r\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\r\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\r\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\r\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\r\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\r\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\r\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\r\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\r\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\r\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\r\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\r\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\r\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\r\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\r\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\r\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\r\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\r\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\r\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\r\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\r\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\r\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\r\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\r\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\r\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\r\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\r\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\r\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\r\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\r\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\r\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\r\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\r\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\r\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\r\n",
    "stopwords += ['yours', 'yourself', 'yourselves']\r\n",
    "stopwords += ['xe2','x80','x93','max','posner','npr','toggle','hide','caption','enlarge','image']\r\n",
    "\r\n",
    "def removeStopWords(NPR_word_bag, stopwords):\r\n",
    "    return [w for w in NPR_word_bag if w not in stopwords]\r\n",
    "NPRArticle_words = removeStopWords(NPR_word_bag, stopwords)\r\n",
    "print(NPRArticle_words[0:200])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['torey', 'edmonds', 'lived', 'house', 'african', 'american', 'neighborhood', 'east', 'end', 'richmond', 'va', '61', 'years', 'little', 'girl', 'says', 'neighborhood', 'place', 'tidy', 'homes', 'rose', 'bushes', 'fruit', 'trees', 'residents', 'ready', 'access', 'shops', 'like', 'beauty', 'salons', 'movie', 'theaters', 'grocery', 'stores', 'grew', 'says', 'neighborhood', 'went', 'downhill', '1970s', 'stores', 'disappeared', 'return', 'corner', 'shops', 'selling', 'cheap', 'alcohol', 'real', 'food', 'edmonds', 'says', 'houses', 'declined', 'homeowners', 'including', 'parents', 'rejected', 'loans', 'bank', 'loaning', 'says', 'things', 'deteriorate', 'today', 'edmonds', 'neighborhood', 'remains', 'overwhelmingly', 'african', 'american', 'poverty', 'rate', 'nearly', '60', 'neighbors', 'suffer', 'chronic', 'medical', 'conditions', 'like', 'kidney', 'disease', 'diabetes', 'age', 'differently', 'says', 'edmonds', 'works', 'virginia', 'commonwealth', 'university', 'promoting', 'community', 'health', 'lot', 'neighbors', 'health', 'challenges', 'n', 'n', 'n', 'n', 'n', 'n', 'loading', 'n', 'n', 'n', 'n', 'just', 'edmonds', 'neighborhood', 'city', 'city', 'u', 'milwaukee', 'miami', 'researchers', 'disturbing', 'pattern', 'people', 'live', 'neighborhoods', 'subjected', 'discriminatory', 'lending', 'practice', 'called', 'redlining', 'today', 'likely', 'experience', 'shorter', 'life', 'spans', '20', '30', 'years', 'shorter', 'neighborhoods', 'city', 'researchers', 'national', 'community', 'reinvestment', 'coalition', 'university', 'richmond', 'university', 'wisconsin', 'milwaukee', 'analyzed', 'historic', 'redlining', 'maps', '142', 'urban', 'areas', 'u', 'x94', 'maps', 'created', '1930s', 'classified', 'black', 'immigrant', 'communities', 'risky', 'places', 'make', 'home', 'loans', 'compared', 'maps', 'current', 'economic', 'status', 'health', 'outcomes', 'neighborhoods', 'today', 'higher', 'rates', 'poverty', 'shorter', 'life', 'spans', 'higher', 'rates', 'chronic', 'diseases', 'including', 'asthma', 'diabetes', 'hypertension', 'obesity', 'kidney', 'disease']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now we're ready to count our words, and move from an array to a dictionary. \r\n",
    "# Using the functions we've already built in the \"Dictionaries.ipynb\" file, # process your text by building a dictionary that zips words with their frequency, \r\n",
    "# then removes redundancy by storing the data in the \"dictionary\" format.\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "db64136dc4130c2dda380bc0ce411ffaa0317e9a9f99d4dbeea1fc56cb102fad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}