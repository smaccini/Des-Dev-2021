{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Lets's start coding...\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Time for dictionaries!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"And sorting!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For this exercise, you should start by reading in the text file produced from Exercise Three. \r\n",
    "# This text file should already have the HTML code elements removed, and primarily consist of text and other characters that we will remove through our processing. \r\n",
    "# Store the input of your file in a string, and convert the contents to lower case for consistency.\r\n",
    "\r\n",
    "# open the txt file to read it\r\n",
    "f = open(\"NPRarticlecleaned.txt\",\"r\")\r\n",
    "\r\n",
    "# create new variable with contents\r\n",
    "NPRtextWeek4 = f.read()\r\n",
    "print(NPRtextWeek4[0:200])\r\n",
    "f.close()\r\n",
    "\r\n",
    "# covert to lower\r\n",
    "\r\n",
    "NPRcleanedlower = NPRtextWeek4.lower()\r\n",
    "print(NPRcleanedlower[0:200])\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This week, we'll be using functions that we've already defined in our exercises. \r\n",
    "# Try looking back through your code and notes from class. Our first step is to import the regular expressions module and remove all non-alpha-numeric characters from the string, \r\n",
    "# then save the string as an array of words ready for processing. \r\n",
    "# (Use the code in NormalizingText for reference)\r\n",
    "\r\n",
    "# In class, we did these in reverse order, and I am going to stick with the order used in class.\r\n",
    "\r\n",
    "# Creating the bag of words array\r\n",
    "NPR_word_bag = NPRcleanedlower.split()\r\n",
    "print(NPR_word_bag[0:200])\r\n",
    "\r\n",
    "\r\n",
    "# Remove non-alpha-numeric characters\r\n",
    "\r\n",
    "import re\r\n",
    "NPR_word_bag = re.compile(r'\\W+', re.UNICODE).split(NPRcleanedlower)\r\n",
    "print(NPR_word_bag[0:200])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Next, we'll use stop words to remove all the words that we don't want to include in our count. \r\n",
    "# There are suggested words in this week's readings, but you can also generate a custom list based on your topic. Define the stop words in an array, \r\n",
    "# and use a loop to remove any word in your bag of words that also appears on the stop words list (use the examples in Dictionaries for guidance.)\r\n",
    "\r\n",
    "# Using stopwords from Turkel list plus adding a few more that seem to be clutter to my list\r\n",
    "\r\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\r\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\r\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\r\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\r\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\r\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\r\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\r\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\r\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\r\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\r\n",
    "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\r\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\r\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\r\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\r\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\r\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\r\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\r\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\r\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\r\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\r\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\r\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\r\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\r\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\r\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\r\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\r\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\r\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\r\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\r\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\r\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\r\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\r\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\r\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\r\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\r\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\r\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\r\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\r\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\r\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\r\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\r\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\r\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\r\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\r\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\r\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\r\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\r\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\r\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\r\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\r\n",
    "stopwords += ['yours', 'yourself', 'yourselves']\r\n",
    "stopwords += ['xe2','x80','x93','max','posner','npr','toggle','hide','caption','enlarge','image','n','says']\r\n",
    "\r\n",
    "def removeStopWords(NPR_word_bag, stopwords):\r\n",
    "    return [w for w in NPR_word_bag if w not in stopwords]\r\n",
    "NPRArticle_words = removeStopWords(NPR_word_bag, stopwords)\r\n",
    "print(NPRArticle_words[0:200])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now we're ready to count our words, and move from an array to a dictionary. \r\n",
    "# Using the functions we've already built in the \"Dictionaries.ipynb\" file, # process your text by building a dictionary that zips words with their frequency, \r\n",
    "# then removes redundancy by storing the data in the \"dictionary\" format.\r\n",
    "\r\n",
    "def wordsToDictionary(NPRArticle_words):\r\n",
    "    word_freq = [NPRArticle_words.count(word) for word in NPRArticle_words]\r\n",
    "    return dict(list(zip(NPRArticle_words,word_freq)))\r\n",
    "\r\n",
    "\r\n",
    "counted_words = wordsToDictionary(NPRArticle_words)\r\n",
    "print(counted_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Finally, explore what you can learn from this dictionary. Try: \r\n",
    "# A. Sorting your dictionary using our prebuilt method\r\n",
    "\r\n",
    "\r\n",
    "def sortDictionary(counted_words):\r\n",
    "    aux = [(counted_words[key], key) for key in counted_words]\r\n",
    "    aux.sort()\r\n",
    "    aux.reverse()\r\n",
    "    return aux\r\n",
    "\r\n",
    "sorted_counted_words = sortDictionary(counted_words)\r\n",
    "print(sorted_counted_words)\r\n",
    "\r\n",
    "# changed variable to sorted_counted_words\r\n",
    "# was able to add 'says' to stopwords without generating error\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(24, 'redlining'), (21, 'health'), (16, 'richmond'), (15, 'neighborhoods'), (13, 'university'), (13, 'maps'), (11, 'redlined'), (10, 'neighborhood'), (8, 'today'), (8, 'federal'), (8, 'community'), (8, 'communities'), (8, 'areas'), (7, 'researchers'), (7, 'outcomes'), (7, 'edmonds'), (7, 'city'), (6, 'years'), (6, 'urban'), (6, 'study'), (6, 'links'), (5, 'u'), (5, 'policies'), (5, 'milwaukee'), (5, 'meant'), (5, 'likely'), (5, 'life'), (5, 'just'), (5, 'including'), (5, 'housing'), (5, 'government'), (5, 'findings'), (5, 'families'), (5, 'color'), (5, 'american'), (5, '1930s'), (4, 'x94'), (4, 'wisconsin'), (4, 't'), (4, 'social'), (4, 'scholarship'), (4, 'resources'), (4, 'public'), (4, 'policy'), (4, 'nelson'), (4, 'meaning'), (4, 'loans'), (4, 'like'), (4, 'lack'), (4, 'lab'), (4, 'krieger'), (4, 'houses'), (4, 'helped'), (4, 'digital'), (4, 'african'), (3, 'withstand'), (3, 'white'), (3, 'vulnerability'), (3, 'va'), (3, 'studies'), (3, 'stores'), (3, 'shorter'), (3, 'risky'), (3, 'risk'), (3, 'research'), (3, 'reinvestment'), (3, 'refinance'), (3, 'red'), (3, 'racist'), (3, 'racism'), (3, 'practice'), (3, 'poverty'), (3, 'offer'), (3, 'neighbors'), (3, 'natural'), (3, 'national'), (3, 'modern'), (3, 'limited'), (3, 'lenders'), (3, 'investment'), (3, 'impossible'), (3, 'immigrants'), (3, 'human'), (3, 'historic'), (3, 'highest'), (3, 'higher'), (3, 'greater'), (3, 'fell'), (3, 'expectancy'), (3, 'example'), (3, 'disrepair'), (3, 'disasters'), (3, 'difficult'), (3, 'create'), (3, 'cooper'), (3, 'considered'), (3, 'coalition'), (3, 'buy'), (3, 'average'), (3, 'author'), (3, 'analyzed'), (3, 'access'), (3, 'able'), (3, '68'), (2, 'work'), (2, 'ways'), (2, 'trees'), (2, 'time'), (2, 'think'), (2, 'structural'), (2, 'spans'), (2, 'shops'), (2, 'set'), (2, 'segregation'), (2, 'school'), (2, 'say'), (2, 'richardson'), (2, 'result'), (2, 'residential'), (2, 'really'), (2, 'rating'), (2, 'rates'), (2, 'promoting'), (2, 'places'), (2, 'people'), (2, 'pandemic'), (2, 'outlined'), (2, 'opportunities'), (2, 'old'), (2, 'ncrc'), (2, 'mortgage'), (2, 'map'), (2, 'major'), (2, 'loading'), (2, 'live'), (2, 'lending'), (2, 'kind'), (2, 'kidney'), (2, 'involved'), (2, 'interactive'), (2, 'impacts'), (2, 'impact'), (2, 'homeowners'), (2, 'home'), (2, 'history'), (2, 'grocery'), (2, 'going'), (2, 'generations'), (2, 'food'), (2, 'fewer'), (2, 'fairfield'), (2, 'exposure'), (2, 'environment'), (2, 'end'), (2, 'dr'), (2, 'disease'), (2, 'discriminatory'), (2, 'director'), (2, 'diabetes'), (2, 'declined'), (2, 'created'), (2, 'covid'), (2, 'conditions'), (2, 'cities'), (2, 'chronic'), (2, 'choices'), (2, 'black'), (2, '1930'), (2, '19'), (1, 'zip'), (1, 'york'), (1, 'writer'), (1, 'wouldn'), (1, 'worthiness'), (1, 'works'), (1, 'west'), (1, 'went'), (1, 'wealth'), (1, 'way'), (1, 'want'), (1, 'visualizes'), (1, 'virginia'), (1, 'versions'), (1, 'uruguyan'), (1, 'urgent'), (1, 'universities'), (1, 'underlying'), (1, 'undeniable'), (1, 'turn'), (1, 'trust'), (1, 'torey'), (1, 'tidy'), (1, 'things'), (1, 'theaters'), (1, 'tend'), (1, 'systemic'), (1, 'systematically'), (1, 'surveyors'), (1, 'surprise'), (1, 'suffer'), (1, 'suburbs'), (1, 'subjected'), (1, 'stop'), (1, 'status'), (1, 'startling'), (1, 'standard'), (1, 'stage'), (1, 'specific'), (1, 'spaces'), (1, 'sites'), (1, 'shocked'), (1, 'shaping'), (1, 'shaped'), (1, 'severe'), (1, 'selling'), (1, 'segregationist'), (1, 'sanctioned'), (1, 'salons'), (1, 'rose'), (1, 'role'), (1, 'robert'), (1, 'riskiness'), (1, 'revelation'), (1, 'return'), (1, 'retailers'), (1, 'residents'), (1, 'researcher'), (1, 'reported'), (1, 'remains'), (1, 'rejected'), (1, 'regions'), (1, 'reason'), (1, 'real'), (1, 'ready'), (1, 'rated'), (1, 'rate'), (1, 'ranked'), (1, 'range'), (1, 'ramifications'), (1, 'raise'), (1, 'race'), (1, 'quoting'), (1, 'pushed'), (1, 'published'), (1, 'projects'), (1, 'project'), (1, 'professor'), (1, 'private'), (1, 'previously'), (1, 'preterm'), (1, 'predominantly'), (1, 'predictable'), (1, 'predicament'), (1, 'poorer'), (1, 'pollution'), (1, 'point'), (1, 'played'), (1, 'place'), (1, 'pattern'), (1, 'past'), (1, 'partner'), (1, 'particularly'), (1, 'parks'), (1, 'parents'), (1, 'paint'), (1, 'owners'), (1, 'overwhelmingly'), (1, 'outlawed'), (1, 'originated'), (1, 'online'), (1, 'ongoing'), (1, 'official'), (1, 'obesity'), (1, 'number'), (1, 'notes'), (1, 'note'), (1, 'non'), (1, 'new'), (1, 'negroes'), (1, 'negative'), (1, 'nearly'), (1, 'near'), (1, 'nationwide'), (1, 'nancy'), (1, 'movie'), (1, 'moved'), (1, 'mortgages'), (1, 'mold'), (1, 'miles'), (1, 'miami'), (1, 'metropolitan'), (1, 'meier'), (1, 'medical'), (1, 'massachusetts'), (1, 'marked'), (1, 'mapped'), (1, 'makes'), (1, 'make'), (1, 'm'), (1, 'lot'), (1, 'looks'), (1, 'look'), (1, 'long'), (1, 'located'), (1, 'loaning'), (1, 'loan'), (1, 'lived'), (1, 'little'), (1, 'lisa'), (1, 'linking'), (1, 'lingering'), (1, 'levels'), (1, 'left'), (1, 'led'), (1, 'leaving'), (1, 'lead'), (1, 'later'), (1, 'late'), (1, 'large'), (1, 'know'), (1, 'juxtaposes'), (1, 'johns'), (1, 'jason'), (1, 'intergenerational'), (1, 'infiltration'), (1, 'industrial'), (1, 'individual'), (1, 'indicators'), (1, 'important'), (1, 'impacting'), (1, 'immigrant'), (1, 'illustrated'), (1, 'hypertension'), (1, 'house'), (1, 'hotter'), (1, 'hopkins'), (1, 'homes'), (1, 'historian'), (1, 'high'), (1, 'helen'), (1, 'healthy'), (1, 'hazards'), (1, 'harvard'), (1, 'happened'), (1, 'h'), (1, 'grew'), (1, 'green'), (1, 'great'), (1, 'graphics'), (1, 'got'), (1, 'goodbye'), (1, 'global'), (1, 'girl'), (1, 'galeano'), (1, 'fruit'), (1, 'followed'), (1, 'focuses'), (1, 'federally'), (1, 'fares'), (1, 'fair'), (1, 'factors'), (1, 'fabric'), (1, 'explore'), (1, 'explain'), (1, 'experience'), (1, 'exercise'), (1, 'examined'), (1, 'evaluate'), (1, 'especially'), (1, 'epidemiology'), (1, 'epidemiologist'), (1, 'ended'), (1, 'emphasize'), (1, 'effects'), (1, 'eduardo'), (1, 'economic'), (1, 'east'), (1, 'downhill'), (1, 'documents'), (1, 'disturbing'), (1, 'disparities'), (1, 'diseases'), (1, 'discrimination'), (1, 'discouraged'), (1, 'disappeared'), (1, 'disadvantage'), (1, 'digitized'), (1, 'differently'), (1, 'didn'), (1, 'diagnoses'), (1, 'deteriorate'), (1, 'depression'), (1, 'demographics'), (1, 'decisions'), (1, 'day'), (1, 'damage'), (1, 'cut'), (1, 'current'), (1, 'credit'), (1, 'country'), (1, 'corporation'), (1, 'corner'), (1, 'conducive'), (1, 'concentration'), (1, 'concentrate'), (1, 'compared'), (1, 'commonwealth'), (1, 'common'), (1, 'comments'), (1, 'come'), (1, 'combined'), (1, 'coded'), (1, 'code'), (1, 'classified'), (1, 'cited'), (1, 'cheap'), (1, 'characteristics'), (1, 'chan'), (1, 'challenges'), (1, 'cement'), (1, 'cases'), (1, 'cancer'), (1, 'called'), (1, 'bushes'), (1, 'built'), (1, 'build'), (1, 'born'), (1, 'body'), (1, 'bloomberg'), (1, 'births'), (1, 'better'), (1, 'beauty'), (1, 'banks'), (1, 'bank'), (1, 'baked'), (1, 'away'), (1, 'attitudes'), (1, 'asthma'), (1, 'aren'), (1, 'area'), (1, 'arc'), (1, 'americans'), (1, 'allow'), (1, 'alcohol'), (1, 'ago'), (1, 'age'), (1, 'aftermath'), (1, 'actors'), (1, 'act'), (1, '61'), (1, '60'), (1, '30'), (1, '23223'), (1, '21'), (1, '2016'), (1, '20'), (1, '1970s'), (1, '1968'), (1, '142'), (1, '')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Re-trying steps B/C/D\r\n",
    "\r\n",
    "# B. Printing the top five most frequent words from your data\r\n",
    "\r\n",
    "x = 0\r\n",
    "for pair in sorted_counted_words:\r\n",
    "    print(str(pair))\r\n",
    "    x += 1\r\n",
    "    if x==4:\r\n",
    "        break\r\n",
    "\r\n",
    "\r\n",
    "# C. Querying for certain key words and printing their frequency\r\n",
    "\r\n",
    "print(\"Segregation frequency: \" + str(counted_words.get(\"segregation\")))\r\n",
    "print(\"Discrimination frequency: \" + str(counted_words.get(\"discrimination\")))\r\n",
    "\r\n",
    "# D. Comparing the relative frequency of words of interest\r\n",
    "\r\n",
    "def compare_words(word_one, word_two):\r\n",
    "    if not word_one in counted_words or not word_two in counted_words:\r\n",
    "       print(\"Word(s) not found\")\r\n",
    "    elif counted_words[word_one] >  counted_words[word_two]:\r\n",
    "        print(word_one + \" appeared more often\")\r\n",
    "    elif counted_words[word_two] >  counted_words[word_one]:\r\n",
    "        print(word_two + \" appeared more often\") \r\n",
    "    else:\r\n",
    "        print(\"Words occurred with equal frequency\")\r\n",
    "\r\n",
    "compare_words(\"housing\",\"neighborhood\")\r\n",
    "compare_words(\"discrimination\",\"segregation\")\r\n",
    "compare_words(\"neighborhoods\",\"communities\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(24, 'redlining')\n",
      "(21, 'health')\n",
      "(16, 'richmond')\n",
      "(15, 'neighborhoods')\n",
      "Segregation frequency: 2\n",
      "Discrimination frequency: 1\n",
      "neighborhood appeared more often\n",
      "segregation appeared more often\n",
      "neighborhoods appeared more often\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "db64136dc4130c2dda380bc0ce411ffaa0317e9a9f99d4dbeea1fc56cb102fad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}